{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import layout_data.Models.UNet as LDMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (enc1): _EncoderBlock(\n",
      "    (encode): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "      (2): GELU()\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (4): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "      (5): GELU()\n",
      "    )\n",
      "  )\n",
      "  (enc2): _EncoderBlock(\n",
      "    (encode): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "      (2): GELU()\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (4): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "      (5): GELU()\n",
      "    )\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (enc3): _EncoderBlock(\n",
      "    (encode): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      (2): GELU()\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (4): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      (5): GELU()\n",
      "    )\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (enc4): _EncoderBlock(\n",
      "    (encode): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      (2): GELU()\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (4): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      (5): GELU()\n",
      "    )\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (polling): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (center): _DecoderBlock(\n",
      "    (decode): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "      (2): GELU()\n",
      "      (3): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (4): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      (5): GELU()\n",
      "    )\n",
      "  )\n",
      "  (dec4): _DecoderBlock(\n",
      "    (decode): Sequential(\n",
      "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      (2): GELU()\n",
      "      (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (4): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      (5): GELU()\n",
      "    )\n",
      "  )\n",
      "  (dec3): _DecoderBlock(\n",
      "    (decode): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      (2): GELU()\n",
      "      (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (4): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "      (5): GELU()\n",
      "    )\n",
      "  )\n",
      "  (dec2): _DecoderBlock(\n",
      "    (decode): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "      (2): GELU()\n",
      "      (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "      (4): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "      (5): GELU()\n",
      "    )\n",
      "  )\n",
      "  (dec1): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "    (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "    (2): GELU()\n",
      "    (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (4): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "    (5): GELU()\n",
      "  )\n",
      "  (final): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 200, 200])\n"
     ]
    }
   ],
   "source": [
    "model = LDMU.UNet(classes=1, input_channels=1)\n",
    "print(model)\n",
    "x = torch.randn(1, 1, 200, 200)\n",
    "with torch.no_grad():\n",
    "    final = model(x)\n",
    "    print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5460/4281889808.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlayout_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mLDMM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcude\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLDMM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNetUL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/Phys 139 Final Project/hsl-tfp/layout_data/Models/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m \u001b[0;31m#Loading data in the correct way\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExponentialLR\u001b[0m \u001b[0;31m#Exponential learning rates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLightningModule\u001b[0m \u001b[0;31m#Module type to actually run the model. Need to install separately.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#pip install pytorch-lightning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'"
     ]
    }
   ],
   "source": [
    "import layout_data.Models.model as LDMM\n",
    "torch.cude.is_available()\n",
    "\n",
    "model = LDMM.UNetUL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
